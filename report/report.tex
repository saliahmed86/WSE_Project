\documentclass[11pt,twocolumn]{article}
\setlength{\parindent}{0in}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{setspace}
\usepackage{fancyhdr}
\pagestyle{fancy}
\usepackage{graphicx}

\lhead{\footnotesize {CSCI 127 - Introduction to Computer Science (Section 6)} }
\rhead{\footnotesize {Extra Credit Homework 7} }

\textheight 9.0 in 
\textwidth 6.5 true in 
%\hoffset -1.5 true cm
\fancyheadoffset{0 in}
\voffset -2.5 true cm
\newcommand{\myfancy}[1]{\mathop{\mathcal{#1}}}

\begin{document}



\baselineskip 16pt
 %\setcounter{chapter}{1}
 % defining short form------
 \newcommand{\la}{\lambda}
 \newcommand{\si}{\sigma}
 \newcommand{\ol}{1-\lambda}
 \newcommand{\be}{\begin{equation}}
 \newcommand{\ee}{\end{equation}}
 \newcommand{\bea}{\begin{eqnarray}}
 \newcommand{\eea}{\end{eqnarray}}

\setlength{\columnsep}{0.8cm}

 \newcommand{\secHead}[1]{{\bf \underline{#1}}\\}

 \newcommand{\mySection}[2]{

\begin{tabular}{l l}
\begin{minipage}[t]{1in}
\sc{#1 }
\end{minipage}

\begin{minipage}[t]{5in}
#2\\
\end{minipage}
\end{tabular}\\\\
}
\singlespacing


\title{Knowledge Graph \\ Web Search Engine - Final Project Report}
\author{Group 16}
\maketitle


\section{Motivation \& Related Works}
We have gone through all fundamental infrastructures of web search engine during the whole semester so far, now we are trying to get the useful information from the raw data and well organized them to be queried and utilized by the users. We want to structurally learn knowledge from the web context. As search engines become more advanced, they now tend to present a relevant and \emph{specific} peice of information to the users without users having to click on certain links. While this is achieved through knowledge graphs, it is not the only use of knowledge graph. Knowledge graphs may be used to find related documents on the basis of underlying themes instead of simply the frequencies, and even to explore relationships which no one believes exist.\\

In this exploratory project, we have attempted to create knowledge graph using the (semi-)structured information present in Wikipedia. As of the writing of this report, we had extracted ~827K entities and ~2M relationships from 70K articles. With millions of tables, lists and info-boxes, Wikipedia is the largest organized ``database'' of information. We have attempted to explore a small part of this corpus and extract relationships and properties from it. Moreover, our focus has been on extracting most information without human supervision, especially in the form of predefined ontologies and relationships. This of course leads to noise and many errors, but in the long run offers very few restrictions.\\

Our primary motivator for a free text extractor without predefined ontologies and relationships was TextRunner, though this is certainly not the only knowledge extraction system available. KnowItAll, a forerunner to TextRunner, adopted a pure text based extraction approach but was limited in the type of information it could extract. What interested us the most was TextRunner's storage mechanism - it does not use any organized tables to store the extracted entities and relationships; instead it has a simple indexing system, quite unlike a databse of carefully related tables, that stores enitity-relationship-entity triples. The relationships are raw text, which means that an actual single relationship may take several forms in the index.\\

While focusing on pure text for information extraction, TextRunner and similar extraction do not exploit the wealth of information packed in semi-structured forms such as tables and lists. For example, to extract the awards that someone has won is much easier to do if one parses a list under such a heading, rather than look for instances in a text corpus. TextRunner overcomes this by parsing billions of documents, but perhaps such an approach is not necessary. This is what we will try to answer in this project report.


\section{Design \& Architecture}

Our project evolved through various stages before its current form. Some functionality was considered way too advanced for this project, although in some cases it would have greatly boosted the performance of the system. The biggest and most important decision we made was to skip plain text parsing completely - even though this was the \emph{only} form of parsing we considered when we first conceived the idea of this project. At some point we decided that infoboxes, tables and lists are probably better yet rather ignored sources of information. Full text parsing was eventually dropped, mostly done due to time limitation, since this would have a completely separate approach from table/list parsing. The departure from text parsing then led to a different paradigm, one that focuses on extracting and storing entities and their \emph{properties}, which in turn could be other entities. We therefore arrived at a model where we index (entity-property-entity) triples.  \\

%\subsection{Superlative adjective recognition}
{\bf Superlative adjective recognition}\\

One feature that we would have liked to have in this project is the ability for the software to learn the meaning of adjectives. For example, quite often queries are made in the following form: ``longest bridges in the word''. For our system to return the relevant results, it would have to know that ``longest'' should translate to ``order by length''. We intended to do this by scanning list articles in Wikipedia that had titles of the form ``$<$Superlative Adjective$>$ $<$entities$>$ in $<$some constraint$>$''. We were able to successfully match columns with the adjective in the title for most such articles, by looking at the order of elements in each column. However, even though this had a very high success rate for each article, overall the data was noisy to attach meaning to adjectives, and this feature was eventually excluded.\\


%\subsection{Crawling Wikipedia}
{\bf Crawling Wikipedia}\\

Initially, we downloaded the entire Wikipedia corpus, which unpacked to a single XML file. We used some utitlites to parse the file and extract documents in Wikipedia Markup Language (Wikimarkup). However, the extracted content was not of very high quality, in that several new line characters and sometimes even parts of text were missing. Since such characters are critical to correctly parse infoboxes, tables and lists, we instead retrieved documents in Wikimarkup using the Mediawiki API. To extract information in these structures, we wrote the parsers ourselves.\\

{\bf Language processing}\\

We have made significant use of the Stanford Core NLP libraries in this project. We performed POS tagging in the article titles to determine how it should be processed, and perform lemmatization on entities and properties to enhance extraction and search.\\

{\bf Architecture}\\

The system consists of the following components, which are run in phases in a sequence:

\emph{Crawler} (based on some seed ``lists of lists'', this crawls articles within a certain distance of the seed), \emph{Parser and Extractor} (parse the infobox and tables, if any, of individual Wikipedia articles and most of special lists. All parsers get several of structured "relations" to be indexed), \emph{Indexer} (extract important and precise information from the relation as the index, which will be used when serving), \emph{Query Processor}  (Process the query [more work if it is unstructured] and go through our index base to construct response), and \emph{Server} (mediates between various components and the user).




%\begin{itemize}
%\item 
%Crawler: Based on some seed ``lists of lists'', this crawls articles within a certain distance of the seed.
%\item 
%Parser: Parse the infobox and tables, if any, of individual Wikipedia articles and most of special list. For example the List of longest bridges in the world. All parsers get tons of structured "relations" to be indexed.
%\item 
%Indexer: Extract important and precise information from the relation as the index, which will be used when serving.
%\item
%Server: Process the query (more work if it is unstructured) and go through our index base to constitute response. 
%\end{itemize}



%"	Architecture
%1.	Crawler: Crawler.java and some supporting classes
%2.	Parser: Two main parts, List parser and Infobox parser and some supporting %object.
%3.	Indexer: Indexer.java and some other help classes.
%4.	Nlp: Used when dealing with real sentence like list title and handling unstructured query. 
%5.	Query: make instance of the query and handle its language structure.
%6.	Server: serve the user
%7.	Library and API: Stanford-corenlp (http://nlp.stanford.edu/software/corenlp.shtml ), jsoup (http://jsoup.org/ ),  MediaWiki (https://www.mediawiki.org/wiki/MediaWiki )

\section{Implementation}
Details of the each component of the system follow:

\begin{enumerate}

%\item 
%We tried to learn the pattern from the title of a list say "List of longest bridges" to know which column (in this case, length) we should extract. We assume the column, which is in order, is out target because of the superlative adjective here. Unfortunately, because of the relatively small size of our samples, we couldn't be so sure about the effectiveness of this approach. But that's how we implement. 

\item
%The crawler is not too sophisticated. It 
The crawler retrieves Wikipedia documents in Wikimarkup. seeds are lists of lists, for example ``lists of countries", which may in turn have other meaningful lists, such as ``list of countries by GDP". etc.

\item 
%For the parsing part, since most of our targets are formatted in xml/html, an open and a close tag/parenthesis are required. Based on this feature, we used stack to check if we have got an entity and if we reach the end of the content.

Parsing Wikimarkup, specially for (semi-)structured data, requires very careful analysis of the Wikimarkup grammar. The same output can be achieved with several slight variations of the markup, hence there is a variety in the markup code of all articles. The existence of templates and nesting renders the use of regular expressions insufficient, and stack-based parsing has to be employed.\\


Reverse relations give us  precious information, so we have to utilize them carefully. For parsing infoboxes, we assign the infobox type as the property of every reverse relation. For example, the infobox type of Barack Obama is \emph{officeholder} and he has a relation: Barack Obama (entity) $\rightarrow$ officeholder (type) $\rightarrow$ spouse (property) $\rightarrow$ Michelle Obama (entity). Then the reverse relation here is Michelle Obama (entity) $\rightarrow$ spouse (type) $\rightarrow$ officeholder (property) $\rightarrow$ Barack Obama (entity). Basically, type and property is interchangeable in our model and even in Wikipedia. They don't use verb or adjective to describe a relation. For a list, such as the list of volcanoes in Spain, the idea is the same but we get type from entity's infobox. For example, Fuerteventura (entity) $\rightarrow$ volcano (type) $\rightarrow$ country (property) $\rightarrow$ Spain (entity), in which country is extracted from Spain's infobox.


\item
The inverted index follows the same idea as that of a search engine, except that each entry in the posting is a relationship-entity pair, thereby making each posting list a set of triples (in a way similar to that of TextRunner). The algorithm performs a full scan of hte posting list when searching for properties of an entity.\\
\end{enumerate}


{\bf Enhancements}

\begin{itemize}
\item 
To allow better user experience, we applied loose matching when we are handling the query. That is, a query ``population'' will get not only ``population'' but also ``population density'' as the result, which lower the precision. Also we will have to do is applying strict matching. But in this case, we have to make sure we are doing good stemming, synonym handling and other query/index normalizing stuff.

\item 
The indexing structure can be significantly improved. If a user wants to find property $P$ of some entity, then a complete scan of the posting list is unnecessary and slow. Instead, a tree like structure should be adopted that makes it easier to reach a property. A tree-like structure as opposed to simply clustering the properties has the advantage that properties can be made into a hierarchy.

\end{itemize}




\section{Usage}
\begin{enumerate}
\item 
Compile all .java files 
\item 
Execute wseproject.server.KGServer
\item 
Structured query: \url {http://localhost:25816?entity=karachi\&property=population}
\item 
To some degree, unstructured query: \url {http://localhost:25816?query=capital+of+venezuela}
\item 
Our engine supports union and intersection. You can achieve those by putting "or" and "and" in the query. For example, "movies of Matt Damon and Brad Pitt" will give you a list of movie stared by both of them.
\end{enumerate}


\section{Evaluation}

\subsection{Manaul Evaluation}

{\bf Accuracy}

We were interested in the accuracy of our knowledge graph. To evaluate the accuracy, we must compare it to some ground truth. Since there is no computer-processable ground truth of suitable extent, we had to rely on manual evaluation. We issued hundreds of queries and find the ground truth by hand manually. Then compare with our results to compute the accuracy. We define five categories, {person, movie, country, university, river}, with several properties attached to each category. For each category, we generate a sample queries and do the experiment. The table below shows the accuracy.

\begin{
\begin{figure}

\end{figure}




"	Group Member and Contribution
Syed Ali Ahmed: List Parsing, Indexing, and Server Interface
Yen-Tin Liu: Infobox Parsing, Project Report 
Hao Xu: Evaluation, Consulting




\end{document}

